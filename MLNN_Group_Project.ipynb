{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik618aDHH3oe"
      },
      "outputs": [],
      "source": [
        "# To mount the colab notebook to drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-erOgHU2IGcD"
      },
      "outputs": [],
      "source": [
        "# Module Imports\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yQ1zX6kH96O"
      },
      "outputs": [],
      "source": [
        "img = tf.keras.preprocessing.image.load_img('Group_Project_Data/Train/Real/img_0.png')\n",
        "img\n",
        "\n",
        "# Define the image directory path\n",
        "image_dir_train = \"Group_Project_Data/Train\"\n",
        "image_dir_valid = \"Group_Project_Data/Valid\"\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Define the image size\n",
        "img_size = (64, 64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JabmL0wrICWv"
      },
      "outputs": [],
      "source": [
        "# Create the dataset using the image directory path\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=image_dir_train,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"binary\",\n",
        "    batch_size=batch_size,\n",
        "    image_size=img_size,\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    color_mode='grayscale',\n",
        ")\n",
        "\n",
        "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=image_dir_valid,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"binary\",\n",
        "    batch_size=batch_size,\n",
        "    image_size=img_size,\n",
        "    shuffle=True,\n",
        "    seed=123,\n",
        "    color_mode='grayscale',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCIrd5gPQcxG"
      },
      "outputs": [],
      "source": [
        "# Print the class names\n",
        "class_names = train_dataset.class_names\n",
        "print(class_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ErTtBMrQmD-"
      },
      "outputs": [],
      "source": [
        "# Load the training data \n",
        "x_train = []\n",
        "y_train = []\n",
        "for images, labels in train_dataset:\n",
        "    x_train.append(images.numpy())\n",
        "    y_train.append(labels.numpy())\n",
        "\n",
        "x_train = np.concatenate(x_train, axis=0)\n",
        "y_train = np.concatenate(y_train, axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBwT6JgZQoAU"
      },
      "outputs": [],
      "source": [
        "# Load the validation data \n",
        "x_val = []\n",
        "y_val = []\n",
        "for images, labels in val_dataset:\n",
        "    x_val.append(images.numpy())\n",
        "    y_val.append(labels.numpy())\n",
        "\n",
        "x_val = np.concatenate(x_val, axis=0)\n",
        "y_val = np.concatenate(y_val, axis=0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kG8d2UCQpSc"
      },
      "outputs": [],
      "source": [
        "# Rescale the pixel values\n",
        "x_train = x_train/255.\n",
        "x_val = x_val/255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the model architecture with L2 regularization\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1), kernel_regularizer=l2(0.001)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Print the summary of the model architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, batch_size =128, epochs=10)\n",
        "print(history.history.keys)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
